{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie regresji\n",
    "\n",
    "1. $y$ jest liczbą rzeczywistą\n",
    "2. w kodzie to po prostu float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "1. oznaczmy model literą $M$\n",
    "2. parametry $\\theta$, nauczone parametry $\\widehat\\theta$\n",
    "3. w problemie regresji __model jest funkcją__\n",
    "$$M: (\\mathbf{x}, \\theta) \\mapsto \\widehat{\\mathbf{p}}$$\n",
    "4. można oznaczyć nauczony model $M_{\\widehat\\theta}$ i wtedy\n",
    "$$M_{\\widehat\\theta}: \\mathbf{x} \\mapsto \\widehat{p}$$\n",
    "5. $\\widehat{p}$ to rozkład prawdopodobieństwa na $\\mathbb{R}$\n",
    "  * jak model może zwrócić rozkład prawdopodobieństwa?\n",
    "  * w wypadku dyskretnym mógł zwrócić wektor długości $K$, który opisywał wszystkie prawdopodobieństwa po kolei\n",
    "  * tutaj musimy opisać rozkład na całej prostej"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funkcja, która zwraca rozkład ciągły\n",
    "\n",
    "1. __\"umawiamy się\"__ z modelem na pewną __rodzinę rozkładów prawdopodobieństwa__\n",
    "2. model __zwraca liczbę__ lub __wektor liczb__\n",
    "3. my traktujemy tę liczbę (wektor) jako __parametry rozkładu__ z ustalonej rodziny\n",
    "4. parametry rozkładu to __output__ modelu, a __nie parametry__ modelu!\n",
    "5. przykład\n",
    "  * rodzina rozkładów normalnych $\\mathcal{N}(\\mu, 1)$ o stałej wariancji\n",
    "  * model zwraca $\\mu\\in\\mathbb{R}$\n",
    "  * my interpretujemy liczbę $\\mu$ jako rozkład $\\mathcal{N}(\\mu, 1)$\n",
    "6. skąd model \"wie\", że się z nim umówiliśmy?\n",
    "  * model \"wie\", że ma zwrócić liczbę\n",
    "  * model \"wie\", że minimalizujemy funkcję kosztu\n",
    "  * my definiujemy funkcję kosztu - dobieramy ją tak, żeby zgadzała się z naszymi oczekiwaniami"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Likelihood w regresji - Mean Squared Error\n",
    "\n",
    "1. model zwraca liczbę $\\mu\\in\\mathbb{R}$\n",
    "  * $M_{\\widehat\\theta}(\\mathbf{x}_n) = \\mu_n$\n",
    "  * interpretujemy liczbę $\\mu$ jako rozkład normalny $\\mathcal{N}(\\mu, 1)$\n",
    "  * możemy myśleć, że $\\mu_n$ to __\"prawdziwa\"__ etykieta dla $\\mathbf{x}_n$, do której w zbiorze treningowym __dodano szum gaussowski__ $\\mathcal{N}(0, 1)$\n",
    "\n",
    "2. maksymalizujemy __warunkowe__ likelihood\n",
    "  * likelihood $y_n$ pod warunkiem $\\mathbf{x}_n$\n",
    "  * iloczyn po wszystkich parach $(\\mathbf{x}_n, y_n)$ - __założenie I.I.D.__!\n",
    "  * wzór na likelihood\n",
    "$$ \\mathcal{N}(\\mu_1, 1)(y_1)\\cdot\\ldots\\cdot\\mathcal{N}(\\mu_N, 1)(y_N) = \\prod_{n=1}^N \\dfrac{1}{\\sqrt{2\\pi}}\\exp\\big(\\dfrac{-(y_n-\\mu_n)^2}{2}\\big)$$\n",
    "  * negative mean log likelihood\n",
    "$$\\ln[\\sqrt{2\\pi}] + \\dfrac{1}{N}\\sum_{n=1}^N \\dfrac{(y_n-\\mu_n)^2}{2} = const + \\dfrac{1}{2}\\dfrac{1}{N}\\sum_{n=1}^N (y_n-\\mu_n)^2$$\n",
    "\n",
    "3. czyli tak naprawdę minimalizujemy $\\dfrac{1}{N}\\sum_{n=1}^N (y_n-\\mu_n)^2$\n",
    "  * __błąd średniokwadratowy__ (_mean squared error_)\n",
    "  * liczony pomiędzy wszystkimi etykietami $y_n$ a wszystkimi predykcjami $\\mu_n$\n",
    "\n",
    "4. przypominamy model generatywny na rozkładzie normalnym\n",
    "  * model estymuje parametry używając\n",
    "    * estymatora średniej\n",
    "    * obciążonego estymatora wariancji\n",
    "  * jeśli zbiór treningowy ma jeden element $x$, to\n",
    "    * $\\widehat\\mu = x$\n",
    "    * $\\widehat\\sigma = 0$\n",
    "      * ale $\\sigma$ musi być dodatnia!\n",
    "      * \"nieskończenie wąski\" rozkład normalny\n",
    "      * w tym wypadku w regresji nie ma tego problemu, bo ustalamy wariancję $\\sigma^2 = 1$\n",
    "\n",
    "<img src=\"../ml_figures/gaussian_regression.png\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wnioski\n",
    "\n",
    "1. niech pewna wartość $\\mathbf{x}$ występuje __dokładnie raz__ w zbiorze treiningowym\n",
    "  * załóżmy, że na $n$-tej pozycji\n",
    "    * $\\mathbf{x}$ = $\\mathbf{x}_n$\n",
    "  * model uczony maximum likelihood stwierdzi, że\n",
    "    * $\\widehat{p}(y\\mid\\mathbf{x}_n) = \\mathcal{N}(y_n, 1)$\n",
    "    * model __zapamięta__ lub __memoryzuje__ etykietę poprzez zapamiętanie średniej rozkładu normalnego\n",
    "    * prawdziwy rozkład $p(y\\mid\\mathbf{x}_n)$ może być zupełnie inny, ale mamy tylko jedną parę do estymacji\n",
    "\n",
    "2. niech pewna wartość $\\mathbf{x}$ występuje __kilka razy__ w zbiorze treiningowym\n",
    "  * jak teraz wygląda $\\widehat{p}(y\\mid\\mathbf{x}_n)$?\n",
    "    * przypomnieć sobie model generatywny\n",
    "  * czy to jest lepsza estymacja prawdziwego $p(y\\mid\\mathbf{x}_n)$?\n",
    "  * zazwyczaj $\\mathbf{x}$ pochodzi z rozkładu ciągłego\n",
    "    * żadna wartość $\\mathbf{x}$ nie występuje wielokrotnie w zbiorze treningowym\n",
    "\n",
    "3. załóżmy, że etykieta $y$ jest niezależna od $\\mathbf{x}$\n",
    "  * model powinien się nauczyć zwracać zawsze rozkład $p(y)$\n",
    "  * gdyby model wiedział, że etykieta jest niezależna, to mógłby estymować $p(y)$ ze wszystkich $N$ przykładów $y_n$, nie patrząc w ogóle na $\\mathbf{x}_n$\n",
    "  * czego nauczy się model?\n",
    "\n",
    "4. załóżmy, że w __bliskim otoczeniu__ pewnego $\\mathbf{x}_n$ rozkład warunkowy $p(y\\mid\\mathbf{x})\\simeq p(y\\mid\\mathbf{x}_n)$\n",
    "  * niech $\\mathbf{x}_{n_1}, \\mathbf{x}_{n_2}, \\ldots, \\mathbf{x}_{n_M}$ leżą w bliskim otoczeniu $\\mathbf{x}_n$\n",
    "  * gdyby model wiedział, że rozkład $p(y\\mid\\mathbf{x})$ jest tu mniej więcej taki sam, to mógłby estymować go przy użyciu $M$ przykładów $y_{n_1}, \\ldots, y_{n_M}$\n",
    "  * czego nauczy się model?\n",
    "\n",
    "5. co jeśli źle się umówiliśmy z modelem\n",
    "  * prawdziwy rozkład $p(y\\mid\\mathbf{x_n})$ nie jest rozkładem normalnym\n",
    "  * czego nauczy się model, jeśli będzie myślał, że uczy się rozkładu normalnego?\n",
    "    * załóżmy nawet, że ma dużo sampli $y$ dla ustalonego $\\mathbf{x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inne funkcje kosztu\n",
    "\n",
    "1. podamy jeden konkretny przykład - __Mean Absolute Error__\n",
    "  * minimalizujemy $\\dfrac{1}{N}\\sum_{n=1}^N |y_n-\\mu_n|$\n",
    "  * to odpowiada \"umowie\", że model zwraca średnią __rozkładu Laplace'a__\n",
    "    * wzór na gęstość\n",
    "    $$ \\dfrac{1}{2}\\exp\\big(-|x-\\mu|\\big)$$\n",
    "    * czy widać, że negative mean log likelihood to, z dokładnością do stałych, mean absolute error?\n",
    "\n",
    "2. w praktyce można zdefiniować inną \"sensowną\" funkcję kosztu\n",
    "  * \"sensowna\" to taka, która najlepiej rozwiązuje konkretny problem\n",
    "  * z reguły nie trzeba się zastanawiać, jakiemu rozkładowi ona odpowiada\n",
    "  * ale warto się zastanowić\n",
    "  * __na egzaminie obowiązkowo trzeba umieć połączyć funkcję kosztu z rozkładem prawdopodobieństwa__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oznaczenia\n",
    "\n",
    "1. od tej pory będziemy mówili, że dla $\\mathbf{x}_n$ model zwraca $\\widehat{y_n}$\n",
    "2. w zależności od funkcji kosztu $\\widehat{y_n}$ może oznaczać różne rzeczy, np.\n",
    "  * MSE - średnia rozkładu normalnego\n",
    "  * MAE - średnia rozkładu Laplace'a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
