{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motywacja\n",
    "\n",
    "1. model liniowy jest za słaby\n",
    "  * np. klasyfikacja punktów na bliskie i dalekie od zera\n",
    "    * nieliniowa granica decyzyjna\n",
    "2. wiele nieliniowych hipotez, których nie możemy wziąć pod uwagę\n",
    "3. proste rozwiązanie - zdefiniować nieliniową funkcję $\\Phi$\n",
    "$$ \\Phi: \\mathbb{R}^D \\ni \\mathbf{x} \\mapsto \\Phi(\\mathbf{x}) \\in \\mathbb{R}^J$$\n",
    "$$ \\Phi = (\\phi_1, \\ldots, \\phi_J)$$\n",
    "4. $\\phi_j$ to __funkcja bazowa__\n",
    "  * tworzy nową (nieliniową) __cechę__ na podstawie __całego wektora__ $\\mathbf{x}$\n",
    "  * łącznie $J$ nowych cech (zamiast $D$ starych)\n",
    "5. zbyt duże $J$ stwarza problemy\n",
    "  * dobrze jest, gdy $N >> D$ (czyli też $N >> J$)\n",
    "6. $\\Phi$ musi zostać zdefiniowane __przed uczeniem__\n",
    "  * \"udawany\" nieliniowy model\n",
    "  * model tak naprawdę wciąż jest liniowy, bo __nie może sam nauczyć się $\\Phi$__\n",
    "  * model wciąż niekoniecznie będzie mógł znaleźć prawdziwe rozwiązanie\n",
    "  * ale $\\Phi$ pozwala nam dodać do modelu __wiedzę ekspercką__\n",
    "7. model, który uczy się $\\Phi$ - sieć neuronowa\n",
    "  * w teorii - __uniwersalny aproksymator__\n",
    "    * może nauczyć się dowolnego rozkładu prawdopodobieństwa\n",
    "  * w praktyce\n",
    "    * sieć musiałaby być nieskończenie duża\n",
    "    * optymalizacja gradientowa niekoniecznie znajdzie minimum globalne\n",
    "    * a nawet jak znajdzie, to MLE wcale nie jest takie dobre\n",
    "    * ale wciąż to jeden z najlepszych modeli (jeśli umie się go stabilnie trenować)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Przykłady\n",
    "\n",
    "1. często funkcje wielomianowe $\\phi_j(x)=x^j$\n",
    "    * wielomiany są funkcjami globalnymi\n",
    "    * zmiany w jednym obszarze wpływają na wszystkie inne\n",
    "    * użycie wielomianów jest uzasadnione tym, że każda ciągła funkcja może być dowolnie dokładnie aproksymowana wielomianami\n",
    "2. funkcje o postaci $$x_1^{p_1}x_2^{p_2}\\dots x_D^{p_D}$$ z ograniczeniem $p_1+p_2+\\dots+p_D\\leq r$\n",
    "    * razem $$J=\\frac{(D+r)!}{D!\\,r!}$$ funkcji\n",
    "    * $D=10$ i $r=3$ daje $J=286$\n",
    "        * bardzo dużo parametrów do ustalenia\n",
    "        * tylko ostra regularyzacja usunie redundantne\n",
    "3. inne to funkcje __spline__: podział na podobszary i różne wielomiany w nich\n",
    "4. częste są funkcje __lokalne__\n",
    "    * gausowskie \n",
    "    $$\\phi_j(x)=\\exp\\left(-\\frac{(x-\\mu_j)^2}{2\\sigma^2}\\right)$$\n",
    "        * wartości $\\mu_j$\n",
    "            * równo rozłożone\n",
    "            * centroidy klastrów\n",
    "    * sigmoidalne $$\\phi_j(x)=\\sigma\\left(\\frac{x-\\mu_j}{a}\\right)$$\n",
    "5. także Fourierowskie i wavelets\n",
    "6. sklejane, radialne, furierowskie, etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
