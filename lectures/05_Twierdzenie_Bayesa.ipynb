{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wzory\n",
    "\n",
    "### Wzór Bayesa\n",
    "\n",
    "$$p(\\theta\\mid x) = \\dfrac{p(x\\mid \\theta)\\cdot p(\\theta)}{p(x)}$$\n",
    "\n",
    "Jeśli $\\theta$ może przyjmować tylko skończenie wiele wartości, tzn. $\\theta\\in\\{\\theta^1, \\ldots, \\theta^T\\}$, to możemy zapisać:\n",
    "\n",
    "$$p(x) = p(x, \\theta = \\mathrm{\"dowolna\\, wartość\"}) = p(x, \\theta=\\theta^1) + \\ldots + p(x, \\theta=\\theta^T) = p(x\\mid\\theta=\\theta^1)p(\\theta=\\theta^1) + \\ldots + p(x\\mid\\theta=\\theta^T)p(\\theta=\\theta^T)$$\n",
    "\n",
    "i wstawić to do powyższego wzoru (po co? tak będzie wygodniej, o czym przekonamy się za chwilę):\n",
    "\n",
    "$$p(\\theta\\mid x) = \\dfrac{p(x\\mid \\theta)\\cdot p(\\theta)}{p(x\\mid\\theta=\\theta^1)p(\\theta=\\theta^1) + \\ldots + p(x\\mid\\theta=\\theta^T)p(\\theta=\\theta^T)}$$\n",
    "\n",
    "Sam wzór Bayesa wynika wprost z definicji, więc jest dość prosty. Znacznie ciekawsze będzie to, w jaki sposób go użyjemy.\n",
    "\n",
    "### Wzór Bayesa pisany poprawnie\n",
    "\n",
    "Po dwóch stronach równości mamy dwie funkcje dwóch parametrów: $x$ oraz $\\theta$. Jeśli chcemy mieć równość liczb, a nie funkcji, to powinniśmy wybrać jakąś wartość $x$ - niech to będzie np. $x^1$ - oraz wartość $\\theta$ - powiedzmy $\\theta^t$ dla pewnego $t\\in\\{1, \\ldots, T\\}$ - i napisać:\n",
    "\n",
    "$$p(\\theta = \\theta^t\\mid x = x^1) = \\dfrac{p(x = x^1\\mid\\theta=\\theta^t)\\cdot p(\\theta=\\theta^t)}{p(x=x^1)}$$\n",
    "\n",
    "Oczywiście równość zachodzi dla każdej pary dowolnie wybranych wartości $x$ oraz $\\theta$.\n",
    "\n",
    "---\n",
    "### Wzór Bayesa - więcej zmiennych\n",
    "\n",
    "Zamiast $x$ i $\\theta$ oczywiście możemy wstawić do wzoru Bayesa więcej zmiennych. Zasada jest podobna, np.:\n",
    "\n",
    "$$p(\\theta,\\alpha\\mid x,y,z) = \\dfrac{p(x,y,z\\mid \\theta,\\alpha)\\cdot p(\\theta,\\alpha)}{p(x,y,z)}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Uczenie_ maszynowe\n",
    "\n",
    "Wróćmy teraz do pytania z końca poprzedniego notebooka.\n",
    "\n",
    "Przy założeniu I.I.D. wiedza o wartościach elementów ze zbioru treningowego nie ma prawa zmieniać naszej wiedzy na temat wartości ze zbioru testowego. Czy to oznacza, że teoria jest niepoprawna, skoro modele się uczą?\n",
    "\n",
    "Teoria jest poprawna. Powyższe stwierdzenie jest prawdziwe tylko wtedy, kiedy znamy rozkład $p(x)$. Wtedy w oczywisty sposób samplowanie z tego rozkładu nie może zwiększać wiedzy na jego temat, ani też na temat przyszłych sampli. Natomiast dopóki nie znamy $p(x)$, samplowanie z tego rozkładu zwiększa naszą wiedzę na jego temat i w ten sposób, pośrednio, wiedzę o przyszłych samplach.\n",
    "\n",
    "Rozkład prawdopodobieństwa $p(x)$ mówi o niepewności konkretnej wartości wylosowanej z $x$. Ale jak formalnie zapisać naszą niepewność co do rozkładu $p$? Czy to wymaga wprowadzenia nowej meta-teorii, która ma opisywać tę teorię?\n",
    "\n",
    "Na szczęście nie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parametr $\\theta$\n",
    "\n",
    "Wprowadzamy rozkład łączny $p(x,\\theta)$. Musimy go zdefiniować sami, nie korzystając z żadnych danych treningowych. Pomysł polega na tym, że różne ustalone wartości $\\theta$ oznaczają nasze różne hipotezy o __rozkładzie__ $p(x)$. \n",
    "\n",
    "Formalnie, np. hipoteza $\\theta = \\theta^1$ oznacza __warunkowy__ rozkład prawdopodobieństwa\n",
    "\n",
    "$$p(x\\mid \\theta=\\theta^1)$$\n",
    "\n",
    "a nasz obecny stopień przekonania co do poprawności hipotezy $\\theta = \\theta^1$ dany jest jako rozkład __brzegowy__\n",
    "\n",
    "$$p(\\theta=\\theta^1)$$\n",
    "\n",
    "Nie \"wybieramy\" żadnej hipotezy, żeby powiedzieć coś o $x$ - używamy wszystkich hipotez jednocześnie, ważąc je prawdopodobieństwem $p(\\theta)$. Nasza obecna wiedza o $x$ to po prostu drugi rozkład __brzegowy__\n",
    "\n",
    "$$p(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jak zdefiniować $p(x,\\theta)$ bez danych treningowych\n",
    "\n",
    "Przede wszystkim musimy zadbać, żeby uwzględnić w tym rozkładzie wszystkie możliwe hipotezy. Np. jeśli modelujemy rzut niesymetryczną monetą, to $x$ może przyjąć jedną z dwóch wartości: orzeł lub reszka. Natomiast $\\theta$ musi przyjmować nieskończenie wiele wartości z przedziału $[0,1]$, jeśli zinterpretujemy $\\theta=T$ jako \"prawdopodobieństwo wypadnięcia orła wynosi $T$\".\n",
    "\n",
    "Czyli np. definiujemy $p(x=\\mathrm{orzeł}\\mid\\theta=T) = T$ oraz $p(x=\\mathrm{reszka}\\mid\\theta=T) = 1-T$.\n",
    "\n",
    "Brakuje jeszcze $p(\\theta)$ i tu pojawia się problem. Nie ma żadnego \"dobrego\" rozkładu na $\\theta$, musimy po prostu wybrać cokolwiek. Ten rozkład będzie się zmieniał podczas uczenia (o tym za chwilę) i w granicy nieskończenie wielu obserwacji wskaże nam \"najprawdziwszą\" hipotezę, ale jego wartość początkową trzeba \"zgadnąć\".\n",
    "\n",
    "Zły rozkład $p(\\theta)$ może bardzo spowolnić uczenie - jeśli jesteśmy bardzo przekonani do niepoprawnej hipotezy, to musimy obejrzeć bardzo dużo przykładów, żeby w końcu zmienić zdanie.\n",
    "\n",
    "Wracając do pierwszego akapitu - jeśli w rozkładzie $p(x,\\theta)$ nie uwzględnimy wszystkich możliwych hipotez, to nawet przy \"nieskończenie wielu obserwacjach\" możemy mieć niekompletną wiedzę o $x$. Podobnie stanie się, jeśli na początku pewne wartości $\\theta$ będą miały prawdopodobieństwo zero - jeśli $p(\\theta=T)=0$, to jesteśmy pewni, że $\\theta\\neq T$ i żadna liczba obserwacji $x$ nie przekona nas do $\\theta=T$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uczenie bayesowskie - założenie I.I.D. revisited\n",
    "\n",
    "Założenie I.I.D. obowiązuje tylko przy ustalonej hipotezie, to znaczy przy ustalonej wartości $\\theta$. Czyli dla dowolnej wartości $\\theta^k$\n",
    "\n",
    "$$p(x_1=x^1, \\ldots, x_N=x^N\\mid\\theta=\\theta^k) = p(x=x^1\\mid\\theta=\\theta^k)\\ldots p(x=x^N\\mid\\theta=\\theta^k)$$\n",
    "\n",
    "Chcemy w ten sposób powiedzieć: \"nie wiemy, która hipoteza najlepiej opisuje $x$, ale każda z tych hipotez spełnia wg nas założenie I.I.D.\".\n",
    "\n",
    "Kluczowe jest to, że teraz\n",
    "\n",
    "$$p(x_1=x^1, \\ldots, x_N=x^N) \\neq p(x=x^1)\\ldots p(x=x^N)$$\n",
    "\n",
    "czyli być może uda się wnioskować o kolejnych samplach na podstawie poprzednich.\n",
    "\n",
    "Niech $D_{Tr}$ oznacza nasz zbiór treningowy ($D$ jak _dataset_) o rozmiarze $N$. $D_{Tr}$ to tak naprawdę skrócony zapis na:\n",
    "\n",
    "$$ x_1=x^1, x_2=x^2 \\ldots, x_N=x^N$$\n",
    "\n",
    "Chcemy teraz wnioskować o $x_{N+1}$ na podstawie $D_{Tr}$. Załóżmy na chwilę, że $\\theta$ może przyjmować tylko skończenie wiele wartości ze zbioru $\\{\\theta^1, \\ldots, \\theta^K\\}$. To bardzo niepoprawne założenie, ale dzięki temu zobaczymy wzór ze skończoną sumą. W ogólnym przypadku, jeśli mamy szczęście, zamiast sumy jest bardzo skomplikowana całka.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posterior Predictive Distribution (PPD)\n",
    "\n",
    "$$\\begin{align}\n",
    "p(x_{N+1}\\mid D_{Tr}) &= p(x_{N+1}\\mid\\theta=\\theta^1)p(\\theta=\\theta^1\\mid D_{Tr}) + \\ldots + p(x_{N+1}\\mid\\theta=\\theta^K)p(\\theta=\\theta^K\\mid D_{Tr})\\\\\n",
    "&= \\sum_{k=1}^K p(x_{N+1}\\mid\\theta=\\theta^k)p(\\theta=\\theta^k\\mid D_{Tr})\n",
    "\\end{align}$$\n",
    "\n",
    "Rozbijamy $\\theta$ na wszystkie możliwe przypadki. Wiedza o $x_{N+1}$ to średnia wiedza po wszystkich możliwych hipotezach. Ale używamy tu \"zmienionego\" rozkładu brzegowego $p(\\theta)$, ponieważ warunkujemy go zbiorem treningowym!\n",
    "\n",
    "Jak policzyć $p(\\theta=\\theta^k\\mid D_{Tr})$? Użyjmy wzoru Bayesa.\n",
    "\n",
    "$$p(\\theta=\\theta^k\\mid D_{Tr}) = \\dfrac{p(D_{Tr}\\mid\\theta=\\theta^k)p(\\theta=\\theta^k)}{p(D_{Tr})}$$\n",
    "\n",
    "Zamieniliśmy jedno $p$ na trzy różne - czy to pomogło? Przypomnijmy, że mamy zdefiniowany rozkład $p(x,\\theta)$. W praktyce możemy założyć, że łatwo policzyć $p(\\theta)$ oraz $p(x\\mid\\theta)$ dla dowolnej wartości $\\theta$ (z reguły tak się właśnie definiuje ten rozkład łączny - poprzez każdą hipotezę z osobna oraz rozkład na hipotezach). Pokażmy, jak policzyć wyrazy występujące w ułamku:\n",
    "\n",
    "1. $p(D_{Tr}\\mid\\theta=\\theta^k)$ to prawdopodobieństwo zbioru treningowego w hipotezie $\\theta=\\theta^k$:\n",
    "$$p(D_{Tr}\\mid\\theta=\\theta^k) = p(x=x^1\\mid\\theta=\\theta^k)\\ldots p(x=x^N\\mid\\theta=\\theta^k)$$\n",
    "(Uwaga! Założenie I.I.D.)\n",
    "2. $p(\\theta=\\theta^k)$ odczytujemy po prostu z rozkładu brzegowego $p(\\theta)$\n",
    "3. $p(D_{Tr})$ trzeba rozpisać (właśnie dlatego na początku tego notebooka rozpisywaliśmy mianownik):\n",
    "$$p(D_{Tr}) = p(D_{Tr}\\mid\\theta=\\theta^1)p(\\theta=\\theta^1) + \\ldots + p(D_{Tr}\\mid\\theta=\\theta^K)p(\\theta=\\theta^K)$$\n",
    "ale to już umiemy policzyć, patrz punkty 1. i 2.\n",
    "\n",
    "To jest pełna odpowiedź na pytanie kończące poprzedni notebook. Zaobserwowany zbiór treningowy niejako zmienia rozkład brzegowy $p(\\theta)$ - tak naprawdę ten rozkład nie może się zmienić, ale wzory _wyglądają_ tak, jak gdyby został on uwarunkowany zbiorem treningowym - i ten \"zmieniony\" rozkład służy do ważonego uśrednienia wszystkich hipotez o $x_{N+1}$.\n",
    "\n",
    "Zamiast $x_{N+1}$ możemy wstawić dowolnie wiele kolejnych losowań, np. cały zbiór testowy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prior, likelihood, posterior\n",
    "\n",
    "* $p(\\theta=\\theta^k)$ w liczniku to tzw. __prior__ albo __wiedza a priori__, czyli wszystko to, co wiemy o wartościach $\\theta$ bez patrzenia na zbiór treningowy $D_{Tr}$,\n",
    "* $p(D_{Tr}\\mid\\theta=\\theta^k)$ w liczniku to tzw. __likelihood__, który mówi nam, jak bardzo prawdopodobny byłby dataset $D_{Tr}$, gdyby $\\theta^k$ było prawdziwą hipotezą,\n",
    "* $p(\\theta=\\theta^k\\mid D_{Tr})$ po lewej stronie równości to tzw. __posterior__ albo __wiedza a posteriori__, czyli wszystko to, co wiemy o wartościach $\\theta$ po obejrzeniu zbioru treningowego $D_{Tr}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teoria a praktyka\n",
    "\n",
    "Wzór\n",
    "\n",
    "$$p(x_{N+1}\\mid D_{Tr}) = \\sum_{k=1}^K p(x_{N+1}\\mid\\theta=\\theta^k)p(\\theta=\\theta^k\\mid D_{Tr})$$\n",
    "\n",
    "opisuje wszystko, czego możemy się nauczyć. Nie da się wyciągnąć z $D_{Tr}$ jeszcze więcej wiedzy o $x_{N+1}$.\n",
    "\n",
    "Ale:\n",
    "\n",
    "Nie da się uwzględnić wszystkich hipotez w $p(x,\\theta)$. Już w wypadku rzutu monetą obliczenia są skomplikowane, a to przecież jest najprostszy z możliwych przykładów i daleko mu do problemów, które stawiamy przed sieciami neuronowymi.\n",
    "\n",
    "Nie da się sensownie przeliczyć PPD, może za wyjątkiem bardzo specyficznych rozkładów $p(\\theta,x)$.\n",
    "\n",
    "Machine Learning to sztuka znalezienia dobrego i szybkiego przybliżenia powyższego wzoru. Omówmy dwa najpopularniejsze podejścia. Niestety, oba zakładają \"wybór\" jednej z hipotez."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Maximum a Posteriori (MAP)\n",
    "\n",
    "Szukamy takiego $\\widehat\\theta$, które zmaksymalizuje wartość liczbową posteriora:\n",
    "\n",
    "$$\\widehat\\theta = \\operatorname{arg}\\max_{\\theta^k} p(\\theta=\\theta^k\\mid D_{Tr})$$\n",
    "\n",
    "a następnie \"wybieramy\" to jedno $\\widehat\\theta$ i tylko na jego podstawie dokonujemy predykcji:\n",
    "\n",
    "$$p(x_{N+1}\\mid D_{Tr}) \\cong p(x_{N+1}\\mid\\theta=\\widehat\\theta)$$\n",
    "\n",
    "innymi słowy liczymy na to, że:\n",
    "\n",
    "$$p(\\theta=\\widehat\\theta\\mid D_{Tr})\\simeq1$$\n",
    "\n",
    "i wtedy:\n",
    "\n",
    "$$\\sum_{k=1}^K p(x_{N+1}\\mid\\theta=\\theta^k)p(\\theta=\\theta^k\\mid D_{Tr}) \\cong p(x_{N+1}\\mid\\theta=\\widehat\\theta)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Maximum Likelihood Estimator (MLE)\n",
    "\n",
    "Szukamy takiego $\\theta^k$, które zmaksymalizuje wartość liczbową likelihoodu:\n",
    "\n",
    "$$\\widehat\\theta = \\underset{\\theta^k}{\\arg\\max}\\;p\\,(D_{Tr}\\mid\\theta=\\theta^k)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overfitting,\n",
    "\n",
    "czyli sytuacja, w której model zapamiętuje zbiór treningowy, ale nie umie potem wykorzystać tej wiedzy do predykcji na zbiorze testowym. Jest to cena, jaką płacimy za zastąpienie PPD przez MLE. Overfitting __nie występuje__ w przypadku PPD - to jest wzór dokładny, więc nic się nie może \"zepsuć\".\n",
    "\n",
    "Pojęcie overfittingu pojawi się jeszcze wielokrotnie na wykładzie i ćwiczeniach - omówimy je dokładniej po wprowadzeniu konkretnych modeli. Zobaczymy wtedy, dlaczego jest to zjawisko charakterystyczne właśnie dla MLE."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
