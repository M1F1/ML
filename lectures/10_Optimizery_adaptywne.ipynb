{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uwaga\n",
    "\n",
    "1. W oryginalnych pracach dotyczących niektórych z poniższych metod pojawiają się matematyczne \"akrobacje\", które polegają na przykład na definiowaniu \"pierwiastka ze zdiagonalizowanej macierzy produktu zewnętrznego historii gradientów\". Proszę absolutnie nie myśleć w ten sposób o tych optimizerach!\n",
    "\n",
    "\n",
    "2. W tym notebooku wszystkie operacje na wektorach są zdefiniowane _element-wise_, zgodnie z regułami pakietu numpy. Na przykład:\n",
    "    * kwadrat wektora to wektor kwadratów elementów (numpy.square)\n",
    "    * pierwiastek z wektora to wektor pierwiastków z elementów (numpy.sqrt)\n",
    "    * suma, różnica, iloraz, iloczyn - analogicznie\n",
    "    * suma wektora i liczby oznacza dodanie tej liczby do każdej współrzędnej"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adagrad\n",
    "\n",
    "#### Parametry\n",
    "* $\\eta$ - learning rate (typowe wartości: od 0.001 do 0.01)\n",
    "* $\\epsilon$ - zapobiega dzieleniu przez zero (zazwyczaj: $10^{-8})$\n",
    "\n",
    "#### Parametry wewnętrzne\n",
    "* $h^{(t)}$ - wektor sumujący kwadraty gradientów do czasu $t$, wymiar taki sam jak $\\theta$\n",
    "\n",
    "#### Inicjalizacja\n",
    "\n",
    "* $h^{(0)} = \\mathbf{0}$\n",
    "\n",
    "#### Update\n",
    "\n",
    "1. $h^{(t+1)} = h^{(t)} + (\\nabla L(\\theta^{(t)}))^2$\n",
    "2. $\\theta^{(t+1)}=\\theta^{(t)} - \\eta \\dfrac{\\nabla L(\\theta^{(t)})}{\\sqrt{h^{(t+1)} + \\epsilon}}$\n",
    "\n",
    "#### Dyskusja\n",
    "\n",
    "1. dzielenie ma __znormalizować__ gradient\n",
    "2. normalizacja oddzielnie dla:\n",
    "    * każdej współrzędnej gradientu\n",
    "    * czyli dla każdej współrzędnej $\\theta$\n",
    "    * czyli dla każdego parametru modelu\n",
    "3. $h$ jest sumą kwadratów, więc trzeba w mianowniku wziąć pierwiastek - bez pierwiastka działa słabo\n",
    "4. __akumulacja__ kwadratów gradientów\n",
    "    * $h$ to suma, a nie średnia\n",
    "    * gdyby gradienty były stałe, mianownik rósłby proporcjonalnie do $\\sqrt{t}$\n",
    "    * w praktyce bardzo maleje $\\Delta\\theta$, model może __przestać się uczyć__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSProp\n",
    "\n",
    "#### Parametry\n",
    "* $\\eta$ - learning rate (typowe wartości: od 0.001 do 0.01)\n",
    "* $\\gamma$ - współczynnik średniej kroczącej (zazwyczaj: 0.9)\n",
    "* $\\epsilon$ - zapobiega dzieleniu przez zero (zazwyczaj: $10^{-8})$\n",
    "\n",
    "#### Parametry wewnętrzne\n",
    "* $h^{(t)}$ - wektor średniej kroczącej kwadratów gradientów do czasu $t$, wymiar taki sam jak $\\theta$\n",
    "\n",
    "#### Inicjalizacja\n",
    "\n",
    "* $h^{(0)} = \\mathbf{0}$\n",
    "\n",
    "#### Update\n",
    "\n",
    "1. $h^{(t+1)} = \\gamma h^{(t)} + (1-\\gamma)(\\nabla L(\\theta^{(t)}))^2$\n",
    "2. $\\theta^{(t+1)}=\\theta^{(t)} - \\eta \\dfrac{\\nabla L(\\theta^{(t)})}{\\sqrt{h^{(t+1)} + \\epsilon}}$\n",
    "\n",
    "#### Dyskusja\n",
    "\n",
    "1. __cel__: usunąć problem Adagrad z szybko malejącym $\\Delta\\theta$\n",
    "2. średnia krocząca (__moving average__)\n",
    "    * podobna do zwykłej średniej\n",
    "    * zapomina daleką przeszłość\n",
    "    * nie wymaga pamiętania pełnej historii gradientów\n",
    "    * __ograniczenie akumulowania__ gradientów do okienka, $\\frac{1}{1-\\gamma}$ (stałe) zamiast $\\sqrt{t}$ (rosnące)\n",
    "3. zaproponowane przez Hintona\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adadelta\n",
    "\n",
    "#### Parametry\n",
    "* $\\gamma$ - współczynnik średniej kroczącej (zazwyczaj: 0.95)\n",
    "* $\\epsilon$ - zapobiega dzieleniu przez zero, umożliwia rozpoczęcie uczenia (zazwyczaj: od $10^{-6}$ do $10^{-2}$)\n",
    "\n",
    "#### Parametry wewnętrzne\n",
    "* $h^{(t)}$ - wektor średniej kroczącej kwadratów gradientów do czasu $t$, wymiar taki sam jak $\\theta$\n",
    "* $d^{(t)}$ - wektor średniej kroczącej $\\Delta\\theta$ do czasu $t$, wymiar taki sam jak $\\theta$\n",
    "* trzeba też pamiętać $\\theta^{(t-1)}$\n",
    "\n",
    "#### Inicjalizacja\n",
    "\n",
    "* $h^{(0)} = \\mathbf{0}$\n",
    "* $d^{(0)} = \\mathbf{0}$\n",
    "\n",
    "#### Update\n",
    "\n",
    "1. $h^{(t+1)} = \\gamma h^{(t)} + (1-\\gamma)(\\nabla L(\\theta^{(t)}))^2$\n",
    "2. $\\theta^{(t+1)}=\\theta^{(t)} - \\sqrt{d^{(t)} + \\epsilon} \\dfrac{\\nabla L(\\theta^{(t)})}{\\sqrt{h^{(t+1)} + \\epsilon}}$\n",
    "3. $d^{(t+1)} = \\gamma d^{(t)} + (1-\\gamma)(\\theta^{(t+1)} - \\theta^{(t)})^2$\n",
    "\n",
    "#### Dyskusja\n",
    "\n",
    "1. rozszerzenie RMSProp (ale wymyślone niezależnie jako poprawka Adagrad)\n",
    "2. zastąpienie learning rate przez __wektor__\n",
    "    * learning rate __proporcjonalny do__ średniego $\\Delta\\theta$\n",
    "    * upodobnienie szybkości poprawek do poprawek\n",
    "    * __eliminacja__ stałej uczenia z parametrów\n",
    "3. ważna rola parametru $\\epsilon$\n",
    "    * nie tylko zapobiega dzieleniu przez zero\n",
    "    * umożliwia rozpoczęcie uczenia - $d^{(1)}$ większe od zera\n",
    "    * $\\sqrt{\\epsilon}$ wyznacza __dolne ograniczenie__ $\\sqrt{d^{(t)} +\\epsilon}$ - uczenie nie zatrzymuje się"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam Adaptive Moment Estimation\n",
    "\n",
    "#### Parametry\n",
    "* $\\eta$ - learning rate (zazwyczaj: 0.001)\n",
    "* $\\beta_1$ - współczynnik średniej kroczącej pierwszego momentu (zazwyczaj: 0.9)\n",
    "* $\\beta_2$ - współczynnik średniej kroczącej pierwszego momentu (zazwyczaj: 0.999)\n",
    "* $\\epsilon$ - zapobiega dzieleniu przez zero (zazwyczaj: $10^{-8}$)\n",
    "\n",
    "#### Parametry wewnętrzne\n",
    "* $m^{(t)}$ - wektor średniej kroczącej pierwszego momentu gradientu do czasu $t$, wymiar taki sam jak $\\theta$\n",
    "* $v^{(t)}$ - wektor średniej kroczącej drugiego momentu gradientu do czasu $t$, wymiar taki sam jak $\\theta$\n",
    "\n",
    "#### Inicjalizacja\n",
    "\n",
    "* $m^{(0)} = \\mathbf{0}$\n",
    "* $v^{(0)} = \\mathbf{0}$\n",
    "\n",
    "#### Update\n",
    "\n",
    "1. $m^{(t+1)} = \\beta_1 m^{(t)} + (1-\\beta_1)(\\nabla L(\\theta^{(t)}))$\n",
    "2. $v^{(t+1)} = \\beta_2 v^{(t)} + (1-\\beta_2)(\\nabla L(\\theta^{(t)}))^2$\n",
    "3. $\\widehat m = \\dfrac{m^{(t+1)}}{1-\\beta_1^{t+1}}$\n",
    "4. $\\widehat v = \\dfrac{v^{(t+1)}}{1-\\beta_2^{t+1}}$\n",
    "5. $\\theta^{(t+1)} = \\theta^{(t)} - \\eta\\dfrac{\\widehat m}{\\sqrt{\\widehat v} + \\epsilon}$\n",
    "\n",
    "Uwaga: $\\beta^{t+1}$ to \"$\\beta$ do potęgi $t+1$\", a nie $\\beta$ w czasie $t+1$.\n",
    "\n",
    "#### Dyskusja\n",
    "\n",
    "* __gradient__ adaptowany\n",
    "  * krocząca średnia gradientów (pierwszy moment) - tłumione oscylacje\n",
    "  * krocząca średnia kwadratów gradientów (drugi moment) - normalizacja gradientów\n",
    "* __inicjalizacja__ $v_0$ i $m_0$ na $0$\n",
    "    * średnia krocząca __zbiasowana__ w kierunku zera\n",
    "    * __przeciwdziałanie__\n",
    "        * symbole z \"daszkiem\" wprowadzają poprawkę\n",
    "        * im później (duże $t$), tym mniejsza poprawka\n",
    "* dobrze dobrane parametry domyślne - zazwyczaj nie ma potrzeby ich modyfikacji\n",
    "* jeden z __najlepszych__ optimizerów"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I wiele wiele innych\n",
    "\n",
    "AdaMax, Nadam, AMSGrad, ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
