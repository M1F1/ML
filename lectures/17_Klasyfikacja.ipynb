{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie klasyfikacji\n",
    "\n",
    "1. $y$ przyjmuje jedną z $K$ wartości $\\{c_1, \\ldots, c_K\\}$ - __zbiór klas__\n",
    "    * __klasyfikacja binarna__ gdy $K=2$\n",
    "    * __klasyfikacja wieloklasowa__ (_multi-class_) gdy $K>2$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reprezentacja klas w kodzie\n",
    "\n",
    "1. jako liczba całkowita\n",
    "    * $c_k$ reprezentowane jako liczba $k-1$\n",
    "    * wyłącznie dla wygody, klasy w żaden sposób nie są semantycznie związane z liczbą\n",
    "2. notacja __one-hot__\n",
    "    * $c_k$ reprezentowane jako wektor $y=(0,\\dots,0,1,0,\\dots,0)$, jedynka na $k$-tej pozycji, zero na pozostałych\n",
    "    * używana przez modele gradientowe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "1. oznaczmy model literą $M$\n",
    "2. parametry $\\theta$, nauczone parametry $\\widehat\\theta$\n",
    "3. w problemie klasyfikacji __model jest funkcją__\n",
    "$$M: (\\mathbf{x}, \\theta) \\mapsto \\widehat{\\mathbf{p}}$$\n",
    "4. można oznaczyć nauczony model $M_{\\widehat\\theta}$ i wtedy\n",
    "$$M_{\\widehat\\theta}: \\mathbf{x} \\mapsto \\widehat{\\mathbf{p}}$$\n",
    "5. $\\widehat{\\mathbf{p}}$ to wektor długości $K$, który opisuje nauczony rozkład prawdopodobieństwa $p(y\\mid\\mathbf{x})$\n",
    "    * oznaczmy $\\widehat{\\mathbf{p}} = (\\widehat{p}_1, \\ldots, \\widehat{p}_K)$\n",
    "    * $\\widehat{p}_1$ to przewidywane $p(y=c_k\\mid\\mathbf{x})$\n",
    "    * wszystkie współrzędne $\\widehat{\\mathbf{p}}$ mają nieujemne wartości\n",
    "    * suma współrzędnych $\\widehat{\\mathbf{p}}$ wynosi $1$\n",
    "    * pamiętamy, że dla różnych $\\mathbf{x}$ model zwraca różne $\\widehat{\\mathbf{p}}$\n",
    "\n",
    "#### Klasyfikacja binarna\n",
    "\n",
    "1. zazwyczaj oznaczamy $c_1$ liczbą $0$, a $c_2$ liczbą $1$\n",
    "2. nauczony model może zwracać tylko jedną liczbę\n",
    "$$M_{\\widehat\\theta}: \\mathbf{x} \\mapsto \\widehat{p}$$\n",
    "3. umawiamy się, że ta liczba oznacza prawdopodobieństwo klasy $c_2$\n",
    "    * $\\widehat{p} \\in [0,1]$\n",
    "4. prawdopodobieństwo $c_1$ to $1 - \\widehat{p}$\n",
    "5. $\\widehat{\\mathbf{p}} = (1-\\widehat{p}, \\widehat{p})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Likelihood w klasyfikacji\n",
    "\n",
    "1. podwójne indeksy\n",
    "  * $(\\mathbf{x}_n, y_n)$ to $n$-ty element zbioru treningowego\n",
    "  * $M_{\\widehat\\theta}(\\mathbf{x}_n) = \\widehat{\\mathbf{p}}_n$\n",
    "  * $\\widehat{\\mathbf{p}}_n$ to $n$-ta predykcja (wektor)\n",
    "  * $c_k$ to $k$-ta klasa\n",
    "  * $\\widehat{\\mathbf{p}}_n = (\\widehat{p}_{n,1}, \\ldots, \\widehat{p}_{n,K})$\n",
    "  * $\\widehat{p}_{n,k}$ to $k$-ta współrzędna $n$-tej predykcji\n",
    "  * $\\widehat{p}_{n,k}$ modeluje $p(y_n=c_k\\mid\\mathbf{x}_n)$\n",
    "\n",
    "2. maksymalizujemy __warunkowe__ likelihood\n",
    "  * likelihood $y_n$ pod warunkiem $\\mathbf{x}_n$\n",
    "  * iloczyn po wszystkich parach $(\\mathbf{x}_n, y_n)$ - __założenie I.I.D.__!\n",
    "  * potrzebujemy oznaczenia, które poda indeks klasy przykładu treningowego $y_n$\n",
    "    * niech $c(y_n) = k$ wtedy i tylko wtedy, gdy $y_n = c_k$\n",
    "  * wzór na likelihood\n",
    "$$ \\widehat{p}_{1,c(y_1)}\\cdot\\ldots\\cdot\\widehat{p}_{N,c(y_N)} = \\prod_{n=1}^N \\widehat{p}_{n,c(y_n)}$$\n",
    "  * negative mean log likelihood\n",
    "$$ -\\dfrac{1}{N}\\sum_{n=1}^N \\ln[\\widehat{p}_{n,c(y_n)}]$$\n",
    "\n",
    "3. przypominamy model generatywny na rozkładzie dyskretnym\n",
    "  * uwaga na parametry $\\theta$!\n",
    "    * tam $\\theta$ oznaczało prawdopodobieństwa\n",
    "    * tutaj $\\theta$ kontroluje sposób, w jaki $\\mathbf{x}$ jest przekształcane w prawdopodobieństwa\n",
    "  * maksimum likelihood $(y\\mid\\mathbf{x})$ to maksimum likelihood $y$ dla każdego $\\mathbf{x}$ z osobna\n",
    "    * $y$ może przyjąć skończenie wiele różnych wartości\n",
    "    * wiemy, jak wygląda maximum likelihood $y$ - tak jak w modelu generatywnym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wnioski\n",
    "\n",
    "1. niech pewna wartość $\\mathbf{x}$ występuje __dokładnie raz__ w zbiorze treiningowym\n",
    "  * załóżmy, że na $n$-tej pozycji\n",
    "    * $\\mathbf{x}$ = $\\mathbf{x}_n$\n",
    "  * model uczony maximum likelihood stwierdzi, że\n",
    "    * $\\widehat{p}(y=y_n\\mid\\mathbf{x}_n) = 1$\n",
    "    * model __zapamięta__ lub __memoryzuje__ etykietę\n",
    "    * prawdziwy rozkład $p(y\\mid\\mathbf{x}_n)$ może być zupełnie inny, ale mamy tylko jedną parę do estymacji\n",
    "\n",
    "2. niech pewna wartość $\\mathbf{x}$ występuje __kilka razy__ w zbiorze treiningowym\n",
    "  * jak teraz wygląda $\\widehat{p}(y\\mid\\mathbf{x}_n)$?\n",
    "    * przypomnieć sobie model generatywny\n",
    "  * czy to jest lepsza estymacja prawdziwego $p(y\\mid\\mathbf{x}_n)$?\n",
    "  * zazwyczaj $\\mathbf{x}$ pochodzi z rozkładu ciągłego\n",
    "    * żadna wartość $\\mathbf{x}$ nie występuje wielokrotnie w zbiorze treningowym\n",
    "\n",
    "3. załóżmy, że etykieta $y$ jest niezależna od $\\mathbf{x}$\n",
    "  * model powinien się nauczyć zwracać zawsze rozkład $p(y)$\n",
    "  * gdyby model wiedział, że etykieta jest niezależna, to mógłby estymować $p(y)$ ze wszystkich $N$ przykładów $y_n$, nie patrząc w ogóle na $\\mathbf{x}_n$\n",
    "  * czego nauczy się model?\n",
    "\n",
    "4. załóżmy, że w __bliskim otoczeniu__ pewnego $\\mathbf{x}_n$ rozkład warunkowy $p(y\\mid\\mathbf{x})\\simeq p(y\\mid\\mathbf{x}_n)$\n",
    "  * niech $\\mathbf{x}_{n_1}, \\mathbf{x}_{n_2}, \\ldots, \\mathbf{x}_{n_M}$ leżą w bliskim otoczeniu $\\mathbf{x}_n$\n",
    "  * gdyby model wiedział, że rozkład $p(y\\mid\\mathbf{x})$ jest tu mniej więcej taki sam, to mógłby estymować go przy użyciu $M$ przykładów $y_{n_1}, \\ldots, y_{n_M}$\n",
    "  * czego nauczy się model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bezpośrednie uczenie funkcji dyskryminatywnej\n",
    "\n",
    "1. być może potrzebujemy jedynie modelu, który __podejmuje decyzje__\n",
    "    * czyli wskazuje jedną konkretną wartość $y$\n",
    "2. mając nauczony łatwo zdefiniować funkcję dyskryminatywną\n",
    "3. można od razu uczyć się funkcji dyskryminatywnej\n",
    "    * potencjalnie łatwiej - nie trzeba modelować pełnego rozkładu $p(y\\mid\\mathbf{x})$\n",
    "    * ale w takim razie nie ma likelihood, więc nie można uczyć metodą maximum likelihood\n",
    "    * przykład: SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Klasyfikacja wieloetykietowa\n",
    "\n",
    "1. klasy nie są rozłączne\n",
    "2. $y$ jest __podzbiorem__ $\\{c_1, \\ldots, c_k\\}$\n",
    "3. można np. wytrenować $K$ klasyfikatorów binarnych\n",
    "    * $k$-ty klasyfikator przewiduje, czy $\\mathbf{x}$ ma etykietę $c_k$\n",
    "    * podejście naiwne, nie korzystamy z zależności pomiędzy różnymi etykietami (a to mogłoby pomóc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nieskończenie wiele klas\n",
    "\n",
    "1. czy można zrobić klasyfikator, jeśli $p(y\\mid\\mathbf{x})$ jest dyskretne na __nieskończonym__ zbiorze?\n",
    "2. w jaki sposób model może zwrócić nieskończenie wiele liczb?\n",
    "3. jak poradzić sobie z faktem, że w __skończonym__ zbiorze treningowym nie może wystąpić jednocześnie __nieskończenie wiele__ klas?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
