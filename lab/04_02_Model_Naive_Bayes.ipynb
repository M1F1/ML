{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motywacja\n",
    "\n",
    "1. chcemy wytrenować prosty i szybki klasyfikator\n",
    "2. przypominamy oznaczenia\n",
    "  * $y$ przyjmuje jedną z $K$ wartości $\\{c_1, \\ldots, c_K\\}$\n",
    "  * $(\\mathbf{x}_n, y_n)$ to $n$-ty element zbioru treningowego\n",
    "  * $x_{n,d}$ to $d$-ta cecha $n$-tego elementu zbioru treningowego\n",
    "3. model musi umieć przewidywać $p(y\\mid\\mathbf{x})$\n",
    "4. pomysł - nauczmy się\n",
    "  * $p(\\mathbf{x}\\mid y)$\n",
    "  * $p(y)$\n",
    "  \n",
    "  i skorzystajmy ze wzoru Bayesa\n",
    "\n",
    "$$p(y\\mid\\mathbf{x}) = \\dfrac{p(\\mathbf{x}\\mid y)p(y)}{\\sum_{k=1}^K p(\\mathbf{x}\\mid y=c_k)p(y=c_k)}$$\n",
    "\n",
    "\n",
    "## Uwaga\n",
    "\n",
    "1. ten klasyfikator __nie będzie uczony bayesowsko__\n",
    "  * rozkładów $p(\\mathbf{x}\\mid y)$ oraz $p(y)$ będziemy uczyć się metodą MLE lub MAP\n",
    "  * wzór Bayesa odpowiada tylko za fragment modelu, mianowicie __predykcję__\n",
    "2. tym razem $p(y)$ to prior, $p(\\mathbf{x}\\mid y)$ to likelihood, a $p(y\\mid\\mathbf{x})$ to posterior\n",
    "  * te pojęcia zawsze występują w pewnym kontekście\n",
    "    * prior to zawsze wiedza, którą posiadamy przed wykonaniem wnioskowania bayesowskiego\n",
    "    * likelihood (evidence) to wiedza, którą zdobywamy w trakcie wnioskowania\n",
    "    * posterior to wiedza końcowa\n",
    "  * np. jeśli wnioskujemy o hipotezach $\\theta$, to musimy sami zdefiniować prior\n",
    "  * tutaj wnioskujemy o konkretnym przykładzie testowym\n",
    "    * prior to wstępna wiedza o jego klasie\n",
    "    * nie definiujemy sami tej wiedzy - prior wynika ze zbioru treningowego\n",
    "3. trzeba mieć świadomość, że wzór Bayesa jest bardzo uniwersalny\n",
    "  * można go stosować w różnych kontekstach, nie tylko przy uczeniu modeli\n",
    "  * dlatego trzeba dobrze zrozumieć wzory, bo łatwo się pomylić\n",
    "    * samo zrozumienie \"matematyki\" nie wystarcza - wzory są proste (mnożenie i dzielenie)\n",
    "    * trzeba rozumieć __dlaczego__ jedne zmienne losowe opisuje prior, a inne likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uczenie $p(y)$\n",
    "\n",
    "1. $N$ danych treningowych $y_1, \\ldots, y_N$\n",
    "2. $y$ przyjmuje jedną z $K$ wartości $c_1, \\ldots, c_K$ - rozkład dyskretny\n",
    "3. zazwyczaj $K<<N$\n",
    "4. uczenie metodą MLE działa bardzo dobrze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uczenie $p(\\mathbf{x}\\mid y)$\n",
    "\n",
    "1. musimy wytrenować $K$ rozkładów $p(\\mathbf{x}\\mid y=c_k)$\n",
    "2. dzielimy dane treningowe na $K$ podzbiorów rozłącznych (używając wartości $y_n$)\n",
    "3. bardzo trudne zadanie\n",
    "  * $\\mathbf{x}$ ma $D$ współrzędnych\n",
    "  * prosta estymacja gęstości w $D$ wymiarach jest praktycznie niewykonalna, jeśli $D$ wynosi przynajmniej kilkadziesiąt\n",
    "  * nieformalnie, rozmiar zbioru treningowego powinien być rzędu $N^D$\n",
    "  * _curse of dimensionality_\n",
    "  * skomplikowane modele generatywne działają, bo mają bardzo dobry prior\n",
    "\n",
    "### \"Naiwne\" założenie\n",
    "\n",
    "1. zakładamy, że __współrzędne__ $\\mathbf{x}$ są __warunkowo niezależne__ pod warunkiem klasy $y$\n",
    "$$p(\\mathbf{x}\\mid y)=p(x_1\\mid y)\\cdot\\ldots\\cdot p(x_D\\mid y)$$\n",
    "2. z reguły __bardzo nieprawdziwe założenie__\n",
    "  * w przeciwieństwie np. do I.I.D.\n",
    "    * niezależność przykładów treningowych vs niezależność cech w ramach klasy\n",
    "  * ale __mocno upraszcza trenowanie__ modelu\n",
    "3. teraz uczymy się $DK$ rozkładów $p(x_d\\mid y=c_k)$\n",
    "  * każdy z takich rozkładów opisuje jedną cechę w ramach jednej klasy\n",
    "  * cechy mają rozkład dyskretny lub ciągły na $\\mathbb{R}$ - wymiar 1 zamiast $D$\n",
    "  * mamy dostatecznie dużo przykładów (rzędu $N/K$), żeby uczyć się metodą MLE lub MAP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dwa przykładowe warianty\n",
    "\n",
    "### Bernoulli Naive Bayes\n",
    "\n",
    "1. zakładamy, że $p(x_d\\mid y=c_k)$ ma rozkład dyskretny binarny\n",
    "  * każda cecha przyjmuje wartość 0 albo 1\n",
    "2. ustalmy cechę $d$ i klasę $c_k$\n",
    "  * niech $M_0$ oznacza liczbę przykładów klasy $c_k$, dla których cecha $d$ wynosi zero\n",
    "  * analogicznie $M_1$ to liczba jedynek\n",
    "3. estymator MLE\n",
    "$$\\begin{align}\n",
    "\\widehat{p}(x_d=0\\mid y=c_k) &= \\dfrac{M_0}{M_0+M_1} \\\\\n",
    "\\widehat{p}(x_d=1\\mid y=c_k) &= \\dfrac{M_1}{M_0+M_1}\n",
    "\\end{align}$$\n",
    "  * może spowodować $\\dfrac{0}{0}$ przy obliczaniu posteriora\n",
    "4. użyjmy jednak estymatora MAP\n",
    "$$\\begin{align}\n",
    "\\widehat{p}(x_d=0\\mid y=c_k) &= \\dfrac{M_0 + \\frac12}{M_0 + M_1 + 1} \\\\\n",
    "\\widehat{p}(x_d=1\\mid y=c_k) &= \\dfrac{M_1 + \\frac12}{M_0 + M_1 + 1}\n",
    "\\end{align}$$\n",
    "5. w modelu zapisujemy tylko $\\widehat{p}(x_d=1\\mid y=c_k)$\n",
    "  * przy predykcji korzystamy ze wzoru\n",
    "$$ \\widehat{p}(x_d=0\\mid y=c_k) = 1 - \\widehat{p}(x_d=1\\mid y=c_k) $$\n",
    "6. $KD+K$ parametrów klasyfikatora\n",
    "  * $KD$ na modelowanie $p(\\mathbf{x}\\mid y)$\n",
    "  * $K$ na modelowanie $p(y)$\n",
    "\n",
    "### Gaussian Naive Bayes\n",
    "\n",
    "1. zakładamy, że $p(x_d\\mid y=c_k)$ ma rozkład normalny\n",
    "  * każda cecha jest liczbą rzeczywistą\n",
    "2. uczymy się średniej $\\mu$ i wariancji $\\sigma^2$\n",
    "  * korzystamy z estymatora MLE\n",
    "$$\\begin{align}\n",
    "\\widehat\\mu &= \\dfrac{1}{N_k}\\sum_{n:\\, y_n=c_k} x_n \\\\\n",
    "\\widehat{\\sigma^2} &= \\dfrac{1}{N_k}\\sum_{n:\\, y_n=c_k} (x_n-\\widehat\\mu)^2\n",
    "\\end{align}$$\n",
    "  * $N_k$ to liczba przykładów w klasie $c_k$\n",
    "  * sumujemy tylko po przykładach z klasy $c_k$\n",
    "  * warto zabezpieczyć się przed wyestymowaniem $\\widehat{\\sigma^2}=0$ (dlaczego?)\n",
    "  $$ \\widehat{\\sigma^2} = max\\{\\widehat{\\sigma^2}, \\epsilon\\}$$\n",
    "    $\\epsilon$ równy np. $10^{-6}$\n",
    "3. $2KD+K$ parametrów klasyfikatora\n",
    "  * $2KD$ na modelowanie $p(\\mathbf{x}\\mid y)$\n",
    "  * $K$ na modelowanie $p(y)$\n",
    "\n",
    "### Materiały dodatkowe\n",
    "\n",
    "http://blog.datumbox.com/machine-learning-tutorial-the-naive-bayes-text-classifier/\n",
    "\n",
    "Wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dyskusja\n",
    "\n",
    "### Inne warianty\n",
    "\n",
    "1. możemy zmodyfikować Bernoulliego i dopuścić wieloklasowe cechy\n",
    "  * może być różna liczba dozwolonych wartości dla różnych $d$\n",
    "2. możemy w ogóle dla różnych $d$ brać różne rozkłady - np. raz dyskretne, raz ciągłe\n",
    "3. Multinomial Naive Bayes\n",
    "  * wektory $\\mathbf{x}$ są __zmiennej długości__\n",
    "  * np. wektor opisuje zdanie, a każda współrzędna to słowo\n",
    "4. warto używać MAP zamiast MLE\n",
    "  * w wypadku $p(y)$ raczej nie trzeba\n",
    "  * w wypadku cech musimy się zabezpieczyć przed dzieleniem przez zero\n",
    "5. można próbować uczyć się metodą PPD, ale to raczej strata czasu\n",
    "  * i tak model jest \"zepsuty\" przez \"naiwne założenie\"\n",
    "\n",
    "### Dlaczego nie powinno się tak budować klasyfikatorów\n",
    "1. $p(y)$ łatwe w estymacji\n",
    "2. zazwyczaj $p(\\mathbf{x}\\mid y)$ bardzo trudne\n",
    "3. znacznie prościej uczyć się od razu $p(y\\mid\\mathbf{x})$\n",
    "4. a więc Naive Bayes ma sens tylko wtedy, gdy $p(\\mathbf{x}\\mid y)$ jest łatwe\n",
    "  * wtedy mamy prosty i szybki model\n",
    "  * model jest zrozumiały, interpretowalny przez człowieka"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
